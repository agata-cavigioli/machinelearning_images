{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed Point Tracking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LUKAS-KANADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@0.044] global cap_v4l.cpp:982 open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n",
      "[ERROR:0@0.045] global obsensor_uvc_stream_channel.cpp:156 getStreamChannelGroup Camera index out of range\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m out \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoWriter(output_path, fourcc, fps, (width, height))  \u001b[38;5;66;03m# OUT FOR SAVING\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ret:\n\u001b[1;32m     35\u001b[0m         gray_frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#######################LUKAS-KANADE\n",
    "import numpy as np \n",
    "import cv2\n",
    "import time\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "end_time = 20  # in seconds\n",
    "timer_duration = 5  # in seconds\n",
    "\n",
    "corNum = 30\n",
    "feature_params = dict(maxCorners=corNum, qualityLevel=0.3, minDistance=7, blockSize=7)\n",
    "lk_params = dict(winSize=(15, 15), maxLevel=2, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "color = np.random.randint(0, 255, (corNum, 3)) \n",
    "\n",
    "old_frame = None\n",
    "old_gray = None\n",
    "p0 = None\n",
    "trajectory = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a VideoWriter object to save the video\n",
    "output_path = 'LUKAS-KANADE1.mp4'\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)  # Get the frames per second\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))  # Get the frame width\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))  # Get the frame height\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))  # OUT FOR SAVING\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        if old_frame is None:  # First iteration\n",
    "            old_frame = frame.copy()\n",
    "            old_gray = gray_frame.copy()\n",
    "            p0 = cv2.goodFeaturesToTrack(old_gray, mask=None, **feature_params)\n",
    "            for i in range(p0.shape[0]):\n",
    "                trajectory.append({'trajectory': [], 'color': color[i].tolist(), 'absence_counter': 0})\n",
    "\n",
    "        else:\n",
    "            current_time = time.time()\n",
    "            elapsed_time = current_time - start_time\n",
    "            \n",
    "            if elapsed_time <= timer_duration:\n",
    "                fastest_speed = -1\n",
    "                fastest_index = None\n",
    "                \n",
    "                # Calculate optical flow using Lucas-Kanade method\n",
    "                p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, gray_frame, p0, None, **lk_params)\n",
    "                good_new = p1[st == 1]\n",
    "                good_old = p0[st == 1]\n",
    "\n",
    "                # Calculate and compare movement distances for each point\n",
    "                for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "                    a, b = new.ravel()\n",
    "                    c, d = old.ravel()\n",
    "                    a, b, c, d = int(a), int(b), int(c), int(d)\n",
    "                    movement_distance = np.sqrt((c - a) ** 2 + (d - b) ** 2)\n",
    "\n",
    "                    if movement_distance > fastest_speed:\n",
    "                        \n",
    "                        fastest_speed = movement_distance\n",
    "                        fastest_index = i\n",
    "                    frame = cv2.circle(frame, (a, b), 5, trajectory[i]['color'], -1)\n",
    "\n",
    "            if elapsed_time >= timer_duration and fastest_index is not None:\n",
    "                # Calculate optical flow using Lucas-Kanade method\n",
    "                p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, gray_frame, p0, None, **lk_params)\n",
    "                good_new = p1[st == 1]\n",
    "                if fastest_index < len(good_new):\n",
    "                    a, b = good_new[fastest_index].ravel()\n",
    "                    a, b = int(a), int(b)\n",
    "                    trajectory[fastest_index]['trajectory'].append((a, b))  # Accumulate points for the trajectory\n",
    "    \n",
    "                    frame = cv2.circle(frame, (a, b), 5, trajectory[fastest_index]['color'], -1)\n",
    "    \n",
    "                    # Draw the trajectory on the video\n",
    "                    if len(trajectory[fastest_index]['trajectory']) > 1:\n",
    "                        for i in range(1, len(trajectory[fastest_index]['trajectory'])):\n",
    "                            cv2.line(frame, trajectory[fastest_index]['trajectory'][i - 1], trajectory[fastest_index]['trajectory'][i], trajectory[fastest_index]['color'], 2)\n",
    "\n",
    "            # Update the previous frame and points for the next iteration\n",
    "            old_gray = gray_frame.copy()\n",
    "            p0 = good_new.reshape(-1, 1, 2)\n",
    "\n",
    "            cv2.imshow('Fastest Moving Point', frame)\n",
    "            out.write(frame)\n",
    "\n",
    "            if elapsed_time >= end_time:\n",
    "                break\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == 27:\n",
    "                break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FARNEBACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import cv2\n",
    "import time\n",
    "\n",
    "frame = cv2.VideoCapture(0)\n",
    "status, old_frame = frame.read()\n",
    "prvs = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "hsv_mask = np.zeros_like(old_frame)\n",
    "hsv_mask[..., 1] = 255\n",
    "\n",
    "numPoint = 10\n",
    "\n",
    "# Create a VideoWriter object to save the video\n",
    "output_path = 'FARNEBACK.mp4'\n",
    "fps = frame.get(cv2.CAP_PROP_FPS)  # Get the frames per second\n",
    "width = int(frame.get(cv2.CAP_PROP_FRAME_WIDTH))  # Get the frame width\n",
    "height = int(frame.get(cv2.CAP_PROP_FRAME_HEIGHT))  # Get the frame height\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))  # OUT FOR SAVING\n",
    "\n",
    "def generate_random_color():\n",
    "    return tuple(np.random.randint(0, 256, 3))\n",
    "\n",
    "def retrieve_fastest_points(flow):\n",
    "    fastest_speed = -1\n",
    "    fastest_points = []\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    \n",
    "    hsv_mask[..., 0] = ang * 180 / np.pi / 2\n",
    "    hsv_mask[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    rgb_representation = cv2.cvtColor(hsv_mask, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    # Find the coordinates of the 10 fastest moving points\n",
    "    for i in range(numPoint):  # Repeat 10 times to find top 10 points\n",
    "        minVal, maxVal, minLoc, maxLoc = cv2.minMaxLoc(mag)\n",
    "        fastest_speed = maxVal\n",
    "        #print(fastest_speed)\n",
    "        fastest_points.append(maxLoc)  # Store the fastest point\n",
    "        mag[maxLoc[1], maxLoc[0]] = 0  # Remove this maximum value from further consideration\n",
    "\n",
    "    return fastest_points\n",
    "\n",
    "\n",
    "while status:\n",
    "    ret, img = frame.read()\n",
    "   \n",
    "    next = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    # Compute magnite and angle of 2D vector\n",
    "    \n",
    "    # Assuming you have already defined variables like 'flow', 'hsv_mask', 'img', 'fastest_speed', 'fastest_points'\n",
    "    fastest_points = retrieve_fastest_points(flow)\n",
    "\n",
    "    for point in fastest_points:\n",
    "       cv2.circle(img, point, 5, (0,0,255), -1) \n",
    "    \n",
    "    cv2.imshow('farneback', img)\n",
    "    out.write(img)\n",
    "   \n",
    "    prvs = next\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break \n",
    "\n",
    "            \n",
    "    \n",
    "frame.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 206.2ms\n",
      "Speed: 4.4ms preprocess, 206.2ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 283.4ms\n",
      "Speed: 4.1ms preprocess, 283.4ms inference, 4.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 265.2ms\n",
      "Speed: 7.9ms preprocess, 265.2ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 312.1ms\n",
      "Speed: 3.9ms preprocess, 312.1ms inference, 3.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 302.9ms\n",
      "Speed: 8.7ms preprocess, 302.9ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 239.2ms\n",
      "Speed: 5.7ms preprocess, 239.2ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 243.1ms\n",
      "Speed: 4.4ms preprocess, 243.1ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 246.5ms\n",
      "Speed: 5.0ms preprocess, 246.5ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 221.5ms\n",
      "Speed: 6.6ms preprocess, 221.5ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 261.9ms\n",
      "Speed: 6.1ms preprocess, 261.9ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 266.6ms\n",
      "Speed: 6.4ms preprocess, 266.6ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 222.8ms\n",
      "Speed: 6.0ms preprocess, 222.8ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 242.2ms\n",
      "Speed: 5.8ms preprocess, 242.2ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 232.6ms\n",
      "Speed: 4.7ms preprocess, 232.6ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 240.7ms\n",
      "Speed: 5.6ms preprocess, 240.7ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 230.0ms\n",
      "Speed: 5.7ms preprocess, 230.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 239.4ms\n",
      "Speed: 4.7ms preprocess, 239.4ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 243.0ms\n",
      "Speed: 4.8ms preprocess, 243.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 221.1ms\n",
      "Speed: 3.3ms preprocess, 221.1ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 215.2ms\n",
      "Speed: 6.4ms preprocess, 215.2ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 247.1ms\n",
      "Speed: 3.7ms preprocess, 247.1ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 235.7ms\n",
      "Speed: 4.6ms preprocess, 235.7ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 272.2ms\n",
      "Speed: 3.9ms preprocess, 272.2ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 213.0ms\n",
      "Speed: 3.9ms preprocess, 213.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 210.2ms\n",
      "Speed: 3.5ms preprocess, 210.2ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 204.2ms\n",
      "Speed: 7.0ms preprocess, 204.2ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 228.3ms\n",
      "Speed: 6.4ms preprocess, 228.3ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 236.5ms\n",
      "Speed: 9.9ms preprocess, 236.5ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 213.1ms\n",
      "Speed: 5.5ms preprocess, 213.1ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 422.4ms\n",
      "Speed: 5.9ms preprocess, 422.4ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 216.3ms\n",
      "Speed: 5.9ms preprocess, 216.3ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 269.6ms\n",
      "Speed: 7.1ms preprocess, 269.6ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 252.0ms\n",
      "Speed: 3.5ms preprocess, 252.0ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 223.3ms\n",
      "Speed: 4.9ms preprocess, 223.3ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 220.5ms\n",
      "Speed: 6.4ms preprocess, 220.5ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 241.4ms\n",
      "Speed: 5.4ms preprocess, 241.4ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 224.2ms\n",
      "Speed: 4.5ms preprocess, 224.2ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 193.0ms\n",
      "Speed: 4.4ms preprocess, 193.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 274.8ms\n",
      "Speed: 3.4ms preprocess, 274.8ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 205.7ms\n",
      "Speed: 3.6ms preprocess, 205.7ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 193.0ms\n",
      "Speed: 4.0ms preprocess, 193.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 234.1ms\n",
      "Speed: 3.2ms preprocess, 234.1ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 224.0ms\n",
      "Speed: 4.5ms preprocess, 224.0ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 209.1ms\n",
      "Speed: 3.5ms preprocess, 209.1ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 227.2ms\n",
      "Speed: 4.9ms preprocess, 227.2ms inference, 4.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 240.1ms\n",
      "Speed: 3.6ms preprocess, 240.1ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 238.1ms\n",
      "Speed: 4.5ms preprocess, 238.1ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 226.3ms\n",
      "Speed: 4.5ms preprocess, 226.3ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 229.4ms\n",
      "Speed: 4.1ms preprocess, 229.4ms inference, 5.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 220.1ms\n",
      "Speed: 3.6ms preprocess, 220.1ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 215.8ms\n",
      "Speed: 4.7ms preprocess, 215.8ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Model\n",
    "#model = YOLO('yolov8x-pose-p6.pt')\n",
    "model=YOLO('yolov8n-pose.pt') #más rápido\n",
    "\n",
    "traiettoria = []\n",
    "\n",
    "# Captura desde lawebcam\n",
    "vid = cv2.VideoCapture(0)\n",
    "ret, frame = vid.read()\n",
    "\n",
    "# Create a VideoWriter object to save the video\n",
    "output_path = 'YOLOrealtime.mp4'\n",
    "fps = vid.get(cv2.CAP_PROP_FPS)  # Get the frames per second\n",
    "width = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))  # Get the frame width\n",
    "height = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))  # Get the frame height\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))  # OUT FOR SAVING\n",
    "  \n",
    "while(True):      \n",
    "    # fotograma a fotograma\n",
    "    ret, frame = vid.read()\n",
    "  \n",
    "    # si hay imagen válida\n",
    "    if ret:  \n",
    "        # Perform inference on an image\n",
    "        #results = model(img, stream=True)\n",
    "        puntos =[]\n",
    "   \n",
    "        # Ejecutar seguimiento YOLOv8 en el fotograma, persistiendo los rastreos entre fotogramas\n",
    "        results = model.track(frame)[0]\n",
    "    \n",
    "        for r in results:\n",
    "            kpts = r.keypoints\n",
    "            nk = kpts.shape[1]\n",
    "            #print(kpts)\n",
    "            #for i in range(nk):            \n",
    "            keypoint=kpts.xy[0,10]    \n",
    "            x, y = (int(keypoint[0])),(int(keypoint[1]))\n",
    "            puntos.append([x,y])\n",
    "            # Disegna il punto corrente  \n",
    "            cv2.circle(frame, (x,y),5,(0,255,0),-1)\n",
    "            \n",
    "            # Aggiungi le coordinate alla traiettoria\n",
    "            traiettoria.append((x, y))\n",
    "              \n",
    "            # Disegna la traiettoria     \n",
    "            for i in range(1, len(traiettoria)):        \n",
    "                cv2.line(frame, traiettoria[i-1], traiettoria[i], (0, 0, 255), 2)\n",
    "\n",
    "        # Muestra fotograma # Mostra il frame con la traiettoria\n",
    "        cv2.imshow(\"Traiectoria del Punto\", frame)\n",
    "        out.write(frame)\n",
    "    \n",
    "    # Detenemos pulsado ESC\n",
    "    if cv2.waitKey(20) == 27:\n",
    "        break\n",
    "  \n",
    "# Libera el objeto de captura\n",
    "vid.release()\n",
    "out.release()\n",
    "# Destruye ventanas\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "# Model\n",
    "model = YOLO('yolov8x-pose-p6.pt')\n",
    "#model=YOLO('yolov8n-pose.pt') más rápido\n",
    "# Images\n",
    "video = cv2.VideoCapture(\"video_para_yolo.mp4\")\n",
    "\n",
    "success, frame = video.read()\n",
    "size=(frame.shape[1], frame.shape[0])\n",
    "fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "video_out = cv2.VideoWriter('YOLO2.mp4', fourcc, 30.0, size)\n",
    "\n",
    "# Bucle a través de los fotogramas del video\n",
    "while (success):\n",
    "    puntos =[]\n",
    "   \n",
    "    # Ejecutar seguimiento YOLOv8 en el fotograma, persistiendo los rastreos entre fotogramas\n",
    "    results = model.track(frame)[0]\n",
    "   \n",
    "    for r in results:\n",
    "        kpts = r.keypoints\n",
    "        nk = kpts.shape[1]\n",
    "        for i in range(nk):\n",
    "            keypoint=kpts.xy[0,i]    \n",
    "            x, y = (int(keypoint[0])),(int(keypoint[1]))\n",
    "            puntos.append([x,y])\n",
    "            cv2.circle(frame, (x,y),5,(0,255,0),-1)\n",
    "                \n",
    "    # Mostrar el fotograma anotado\n",
    "    cv2.imshow('salida',frame)\n",
    "    video_out.write(frame)\n",
    "    success, frame = video.read()\n",
    "    # Romper el bucle si se presiona 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "         success = False\n",
    "         break\n",
    "    \n",
    "\n",
    "# Liberar el objeto de captura de video y cerrar la ventana de visualización\n",
    "video.release()\n",
    "video_out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "import torchvision.transforms as T\n",
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "import cv2\n",
    "\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = \"tight\"\n",
    "\n",
    "\n",
    "\n",
    "def plot(imgs, **imshow_kwargs):\n",
    "    if not isinstance(imgs[0], list):\n",
    "        # Make a 2d grid even if there's just 1 row\n",
    "        imgs = [imgs]\n",
    "\n",
    "    num_rows = len(imgs)\n",
    "    num_cols = len(imgs[0])\n",
    "    _, axs = plt.subplots(nrows=num_rows, ncols=num_cols, squeeze=False)\n",
    "    for row_idx, row in enumerate(imgs):\n",
    "        for col_idx, img in enumerate(row):\n",
    "            ax = axs[row_idx, col_idx]\n",
    "            img = F.to_pil_image(img.to(\"cpu\"))\n",
    "            ax.imshow(np.asarray(img), **imshow_kwargs)\n",
    "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "from torchvision.io import read_video\n",
    "\n",
    "frames, _, _ = read_video(\"video_para_yolo.mp4\")\n",
    "frames = frames.permute(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)\n",
    "\n",
    "img1_batch = torch.stack([frames[100], frames[150]])\n",
    "img2_batch = torch.stack([frames[101], frames[151]])\n",
    "\n",
    "plot(img1_batch)\n",
    "\n",
    "def preprocess(batch):\n",
    "    transforms = T.Compose(\n",
    "        [\n",
    "            T.ConvertImageDtype(torch.float32),\n",
    "            T.Normalize(mean=0.5, std=0.5),  # map [0, 1] into [-1, 1]\n",
    "            T.Resize(size=(520, 960)),\n",
    "        ]\n",
    "    )\n",
    "    batch = transforms(batch)\n",
    "    return batch\n",
    "\n",
    "#verificamos el tipo de imagen de entrada\n",
    "print(f\"shape = {img1_batch.shape}, dtype = {img1_batch.dtype}\")\n",
    "\n",
    "#usamos cuda para ir más rapido\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#preposesamos la imágen\n",
    "img1_batch = preprocess(img1_batch).to(device)\n",
    "img2_batch = preprocess(img2_batch).to(device)\n",
    "\n",
    "#verificamos que el tipo de imagen que obtenemos cumple con el tipo que necesita de entrada el modelo\n",
    "print(f\"shape = {img1_batch.shape}, dtype = {img1_batch.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchvision.models.optical_flow import raft_large #se puede cambiar \"raft_large\" por \"raft_small\" para ir más rápido, pero peor resultado\n",
    "from torchvision.models.optical_flow import raft_small\n",
    "\n",
    "model = raft_small(pretrained=True, progress=False).to(device)\n",
    "model = model.eval()\n",
    "\n",
    "list_of_flows = model(img1_batch.to(device), img2_batch.to(device))\n",
    "print(f\"type = {type(list_of_flows)}\")\n",
    "print(f\"length = {len(list_of_flows)} = number of iterations of the model\")\n",
    "predicted_flows = list_of_flows[-1]\n",
    "print(f\"dtype = {predicted_flows.dtype}\")\n",
    "print(f\"shape = {predicted_flows.shape} = (N, 2, H, W)\")\n",
    "print(f\"min = {predicted_flows.min()}, max = {predicted_flows.max()}\")\n",
    "from torchvision.utils import flow_to_image\n",
    "\n",
    "flow_imgs = flow_to_image(predicted_flows)\n",
    "\n",
    "# The images have been mapped into [-1, 1] but for plotting we want them in [0, 1]\n",
    "img1_batch = [(img1 + 1) / 2 for img1 in img1_batch]\n",
    "\n",
    "grid = [[img1, flow_img] for (img1, flow_img) in zip(img1_batch, flow_imgs)]\n",
    "plot(grid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
