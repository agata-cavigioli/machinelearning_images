{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b27d29f-2f89-4893-987d-adf582f0a174",
   "metadata": {},
   "source": [
    "# LUKAS-KANADE\n",
    "\n",
    "This Python script demonstrates the application of the Lucas-Kanade method in real-time motion tracking through a webcam feed. Utilizing libraries like NumPy and OpenCV, it sets up the framework for optical flow analysis, enabling the identification of the fastest moving point within a video stream. Upon initialization, the script configures parameters for video capture and establishes essential variables.\n",
    "\n",
    "Operating within a processing loop, the script systematically processes each frame captured by the webcam. It employs the Lucas-Kanade method, particularly the cv2.calcOpticalFlowPyrLK function, to analyze movement patterns and detect the most rapid motion within a predefined time interval (timer_duration). The visual representation dynamically illustrates the fastest moving point in real-time, marking its position with distinct circles. Beyond the designated time window, it meticulously traces and displays the trajectory of this swift-moving entity, providing an insightful visual record. This script exemplifies the practical implementation of the Lucas-Kanade method, showcasing its capability in real-time motion analysis. Its systematic approach and visual outputs offer a scientific perspective on motion tracking, emphasizing the method's significance in understanding movement within a video context.\n",
    "\n",
    "For the initial 5-second segment of the video, we track multiple relevant points, allowing the user the freedom to select a specific point of interest for tracking. After this period, we dynamically identify and focus on the fastest-moving point among these tracked points, continuing to follow it throughout the remainder of the recording.\n",
    "\n",
    "The script adeptly showcases the Lucas-Kanade method's prowess in real-time motion analysis. The visual representation effectively tracks and highlights the fastest moving point with precision. However, it's noteworthy that the tracking mechanism might exhibit sensitivity to noise within the video feed. Despite this sensitivity, the script provides a comprehensive illustration of motion tracking, emphasizing the Lucas-Kanade method's scientific significance in dissecting movement dynamics within a video context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7843779d-e15f-4863-b853-39f485371912",
   "metadata": {},
   "source": [
    "# FARNEBACK\n",
    "\n",
    "This code employs the Farneback method in OpenCV for real-time motion tracking via a webcam feed. Utilizing NumPy and CV2 libraries, it delves into optical flow analysis. The script initializes by capturing the initial frame from the webcam and establishing essential elements for visualization. Grayscale conversion and an HSV mask set the groundwork for motion point representation.\n",
    "\n",
    "Through the Farneback method, optical flow calculation unfolds, revealing the top ten fastest moving points within the frame. These points are distinctly marked with red circles, showcasing their rapid trajectories in the visual output.\n",
    "\n",
    "However, the script exhibits sensitivity to noise, affecting the precision in retrieving these top points. Despite the method's sophistication, noise interference slightly compromises the accuracy of tracking these swift-moving entities. This demonstration underscores the Farneback method's capabilities and limitations in motion tracking, offering a glimpse into dynamic movement visualization while hinting at the challenges posed by noise interference in precise motion detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd850e19-e055-4dd4-bb66-8da11260e371",
   "metadata": {},
   "source": [
    "# YOLO\n",
    "\n",
    "This Python script employs YOLOv8 from the Ultralytics library alongside PyTorch and OpenCV for real-time human pose tracking using a webcam feed. The code initializes the 'yolov8n-pose.pt' model configuration, optimized for speed in detecting human poses. Operating within a loop, the script processes each frame captured by the webcam. It utilizes YOLOv8 to track human keypoints persistently across frames, marking these points with green circles on the frame. Additionally, it traces and displays the trajectories of these keypoints, revealing movement patterns.\n",
    "\n",
    "However, it's essential to note potential precision limitations in detecting and tracking keypoints, which could impact accuracy under certain conditions. The script concludes by properly releasing resources and closing windows, concluding this exploration into real-time human pose tracking. This code showcases YOLOv8's capability in real-time pose estimation using a webcam feed. While offering visual representations of detected keypoints and their trajectories, it acknowledges potential challenges in maintaining precision, providing a glimpse into the complexities of real-time pose tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64edae4d-057d-418a-a6c1-3723ee1b589d",
   "metadata": {},
   "source": [
    "# RAFT\n",
    "\n",
    "This script leverages the RAFT (Recurrent All-Pairs Field Transforms) algorithm for human pose estimation. It utilizes the Ultralytics library, specifically the YOLOv8 model configured for pose detection ('yolov8x-pose-p6.pt' model configuration). The code starts by initializing the YOLOv8 model, setting it up for human pose detection using the RAFT algorithm. It then accesses a video file ('people2.mp4') as the input source for processing. However, due to performance issues impacting the algorithm's speed, the code processes only two frames from the provided video file. This limitation restricts the algorithm's execution, acknowledging the sluggishness of its computations. Within the loop, the script iterates through the frames, executing RAFT (implemented through YOLOv8) on each frame to detect human pose keypoints. Detected keypoints are annotated on the frames with green circles, visualizing their positions.\n",
    "\n",
    "While the script aims for continuous processing, the 'q' key press breaks the loop, ending the execution prematurely due to the constrained processing capacity of the RAFT algorithm. Finally, the script concludes by releasing video capture resources and closing display windows. This explanation emphasizes the code's implementation of the RAFT algorithm for human pose detection and acknowledges the restricted execution to just two frames due to performance limitations, enabling an understanding of the code's behavior in processing the video file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9cfaaf-7201-4f23-a423-59e16157cb89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
