{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7605a287-69a2-4692-bdad-da8cfe807a01",
   "metadata": {
    "id": "7605a287-69a2-4692-bdad-da8cfe807a01"
   },
   "source": [
    "# Redes Convolucionales con Pytorch\n",
    "\n",
    "_Clasificación de imágenes_\n",
    "\n",
    "<b>Objetivos:</b>\n",
    "1. Clasifición de números con MNIST\n",
    "2. Comparar una red fully connected con una Convolucional\n",
    "3. Utilizar ResNet18 desde cero\n",
    "4. Utilizar ResNet18 con Transfer Learning\n",
    "\n",
    "<b>Paquetes a instalar:</b>\n",
    "Pytorch, Torchvision, cudnn, cudatoolkit, matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04b04ecd-3d53-4e46-8469-f773cdb28476",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "04b04ecd-3d53-4e46-8469-f773cdb28476",
    "outputId": "92b7049c-62b2-4ead-b16b-bc46bc3558eb",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Inicialización\n",
    "import torch\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c82c37-01dd-4ca2-95c4-68ad954216b7",
   "metadata": {
    "id": "89c82c37-01dd-4ca2-95c4-68ad954216b7"
   },
   "source": [
    "## Dataset de MNIST\n",
    "- Imágenes de 28x28 píxeles\n",
    "- 60.000 imágenes de training\n",
    "- 10.000 imágenes de test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b51b0cd2-8050-4111-b75e-fc8056a7e4b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "id": "b51b0cd2-8050-4111-b75e-fc8056a7e4b4",
    "outputId": "54628a86-e790-4459-819a-71ec23a62356",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definición de constantes \n",
    "import os\n",
    "\n",
    "# esto es para que funcione matplotlib \n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 30\n",
    "IMAGE_SIZE = 28\n",
    "DATASET_DIR = 'data'\n",
    "MODELS_DIR = 'models/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaf2b91f-db3a-48c7-9e6b-a17ff7222063",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 9912422/9912422 [00:00<00:00, 23742503.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 28881/28881 [00:00<00:00, 14310182.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 1648877/1648877 [00:00<00:00, 7598277.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 4542/4542 [00:00<00:00, 5765898.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([256, 1, 28, 28])\n",
      "Shape of y: torch.Size([256]) torch.int64\n",
      "Classes: {'0 - zero': 0, '1 - one': 1, '2 - two': 2, '3 - three': 3, '4 - four': 4, '5 - five': 5, '6 - six': 6, '7 - seven': 7, '8 - eight': 8, '9 - nine': 9}\n",
      "Num classes: 10\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cargamos dataset\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor, Compose\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# importamos los conjuntos de training y test\n",
    "training = datasets.MNIST(DATASET_DIR, train=True, download=True,\n",
    "                          transform=ToTensor())\n",
    "testing = datasets.MNIST(DATASET_DIR, train=False, download=True,\n",
    "                      transform=ToTensor())\n",
    "\n",
    "# creamos los Dataloader para cada conjunto\n",
    "train_dataloader = DataLoader(training, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(testing, batch_size=BATCH_SIZE)\n",
    "\n",
    "# estructura de los batches\n",
    "X, y = next(iter(train_dataloader))\n",
    "print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "\n",
    "classes = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "num_classes = len(testing.class_to_idx)\n",
    "print('Classes:', testing.class_to_idx)\n",
    "print('Num classes:', num_classes)\n",
    "\n",
    "# mostramos las 9 primeras imágenes\n",
    "i=1\n",
    "for img,j in zip(X[:9],y[:9]):\n",
    "    plt.subplot(3,3,i)\n",
    "    plt.imshow(img.reshape((IMAGE_SIZE,IMAGE_SIZE)))\n",
    "    plt.title(j.item())\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca720adf-7872-48a4-b8c5-eafeaad60bda",
   "metadata": {
    "id": "ca720adf-7872-48a4-b8c5-eafeaad60bda"
   },
   "source": [
    "## Entrenamiento con fully connected network\n",
    "- Creación de una red con 2 capas ocultas\n",
    "- Estructura: entrada 784, oculta1 512, oculta2 512, salida 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9e527b6-ec7e-42ff-afec-574fe7993aac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d9e527b6-ec7e-42ff-afec-574fe7993aac",
    "outputId": "a5fe58b9-4f7b-4791-8dde-f6979c8a01b6",
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchinfo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Definimos el modelo\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchinfo\u001b[39;00m  \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# creamos la red feed-forward\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFullyConnected\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchinfo'"
     ]
    }
   ],
   "source": [
    "# Definimos el modelo\n",
    "from torch import nn\n",
    "from torchinfo  import summary\n",
    "\n",
    "# creamos la red feed-forward\n",
    "class FullyConnected(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FullyConnected,self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(IMAGE_SIZE*IMAGE_SIZE, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = FullyConnected().to(device)\n",
    "\n",
    "summary(model,\n",
    "        input_size=(BATCH_SIZE, 1, IMAGE_SIZE, IMAGE_SIZE),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hyYTATL08AKo",
   "metadata": {
    "id": "hyYTATL08AKo"
   },
   "source": [
    "### Procedimientos de training y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef97894-12e6-4e65-abe0-f23fedef43d2",
   "metadata": {
    "id": "8ef97894-12e6-4e65-abe0-f23fedef43d2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# definimos la métrica y el optimizador\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# función de entrenamiento\n",
    "def train(dataloader, model, loss_fn, optimizer, losses, accuracy):\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            current = (batch + 1) * len(X)\n",
    "            train_acc = 100*correct / size\n",
    "            losses.append(loss.item())\n",
    "            print(f\"Accuracy: {train_acc:>0.1f}%, Avg loss: {loss.item():>8f} [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    train_acc = 100*correct/size\n",
    "    accuracy.append(train_acc)\n",
    "\n",
    "\n",
    "# función de test\n",
    "def test(dataloader, model, loss_fn, losses, accuracy):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    test_acc = 100*correct/size\n",
    "    losses.append(test_loss)\n",
    "    accuracy.append(test_acc)\n",
    "    print(f\"Test Error: \\n Accuracy: {test_acc:>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PunFWQVFROUP",
   "metadata": {
    "id": "PunFWQVFROUP"
   },
   "source": [
    "### Bucle principal para Fully Connected Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TkoXNGsTRCio",
   "metadata": {
    "id": "TkoXNGsTRCio",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bucle de entranamiento\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "for t in range(EPOCHS):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer, train_loss, train_acc)\n",
    "    test(test_dataloader, model, loss_fn, test_loss, test_acc)\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "torch.save(model.state_dict(), MODELS_DIR+\"model_ff.pth\")\n",
    "print(\"Model saved model_ff.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XTcIwhPK8OEE",
   "metadata": {
    "id": "XTcIwhPK8OEE"
   },
   "source": [
    "### Gráficas de precisión y pérdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a05430a-8f95-4ba2-8f65-45a45fca8003",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "id": "8a05430a-8f95-4ba2-8f65-45a45fca8003",
    "outputId": "2917aec7-bb7b-4277-e8bb-c25d1c1d1c8d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_graphics(train_loss, train_acc, test_acc):\n",
    "\n",
    "    # Dibujamos las gráficas\n",
    "    x = range(len(train_loss))\n",
    "    plt.figure()\n",
    "    plt.plot(x,train_loss, color='blue')\n",
    "    plt.legend(['Train Loss'], loc='upper right')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.savefig('train_loss.svg')\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(train_acc, color='blue')\n",
    "    plt.plot(test_acc, color='red')\n",
    "    plt.legend(['Train Accuracy', 'Test Accuracy'], loc='lower right')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.savefig('Accuracy.svg')\n",
    "    \n",
    "draw_graphics(train_loss, train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_t1FJ9lV-EKB",
   "metadata": {
    "id": "_t1FJ9lV-EKB"
   },
   "source": [
    "### Visualización de dígitos mal clasificados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GTQm7Mq7-MuQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GTQm7Mq7-MuQ",
    "outputId": "8476ba4b-df13-493b-acea-76c7a97f84c2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Función para buscar incorrectos y dibujarlos\n",
    "def draw_incorrect(model, dataloader, num=3):\n",
    "    for i in range(num):\n",
    "        X, y = next(iter(dataloader))\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        model.to(device)\n",
    "        pred = model(X)\n",
    "        incorrect = (pred.argmax(1) != y)\n",
    "        for img,l,d in zip(X[incorrect],y[incorrect],pred[incorrect].argmax(1)):\n",
    "            img, l = img.to('cpu'), l.to('cpu')\n",
    "            plt.imshow(img.reshape((IMAGE_SIZE,IMAGE_SIZE)))\n",
    "            plt.title(f\"{l.item()} - {d.item()}\")\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "        \n",
    "# Contamos el número de elementos que hay de cada dígito\n",
    "count = np.zeros(10, np.int32)\n",
    "for _,y in test_dataloader:\n",
    "    np.add.at(count, y, 1)\n",
    "\n",
    "print(\"Numero elementos por digito:\", count)\n",
    "print(\"Total:\", count.sum())\n",
    "\n",
    "draw_incorrect(model, test_dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adba8358-2845-4494-8701-c264ac794c8e",
   "metadata": {
    "id": "adba8358-2845-4494-8701-c264ac794c8e"
   },
   "source": [
    "## Entrenamiento con redes convolucionales simples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a40d903-85c7-40dc-bd4d-22f2c92260b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ConvNet1\n",
    "- 2 Bloques Convolucionales\n",
    "- 1 Fully-connected\n",
    "\n",
    "### ConvNet2\n",
    "- 2 Bloques Convolucionales\n",
    "- 1 Fully-connected\n",
    "- BatchNormalization\n",
    "\n",
    "### ConvNet3\n",
    "- 3 Bloques Convolucionales\n",
    "- 1 Fully-connected\n",
    "- BatchNormalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529d9f15-c71d-4c55-98fe-718fd7bf9eb4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "529d9f15-c71d-4c55-98fe-718fd7bf9eb4",
    "outputId": "bf7b31fc-84d8-4caa-9d02-7ea8c97997d0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definimos las redes convolucionales\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "\n",
    "def bloque_conv(in_features, out_features):\n",
    "    return nn.Sequential(\n",
    "            nn.Conv2d(in_features, out_features, 3, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "def bloque_conv_norm(in_features, out_features):\n",
    "    return nn.Sequential(\n",
    "            nn.Conv2d(in_features, out_features, 3, 1),\n",
    "            nn.BatchNorm2d(out_features),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "class ConvNet1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet1, self).__init__()\n",
    "        self.conv1 = bloque_conv(1, 32)\n",
    "        self.conv2 = bloque_conv(32, 64)\n",
    "        self.linear_out = nn.Sequential(\n",
    "            #nn.Dropout(0.25),\n",
    "            nn.Flatten(),\n",
    "            #nn.Linear(1600, 128),\n",
    "            #nn.ReLU(),\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(1600, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        logits = self.linear_out(x)\n",
    "        return logits\n",
    "\n",
    "    \n",
    "class ConvNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet2, self).__init__()\n",
    "        self.conv1 = bloque_conv_norm(1, 32)\n",
    "        self.conv2 = bloque_conv_norm(32, 64)\n",
    "        self.linear_out = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1600, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        logits = self.linear_out(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class ConvNet3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet3, self).__init__()\n",
    "        self.conv1 = bloque_conv_norm(1, 32)\n",
    "        self.conv2 = bloque_conv_norm(32, 64)\n",
    "        self.conv3 = bloque_conv_norm(64, 128)\n",
    "        self.linear_out = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        logits = self.linear_out(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e793a5db-df59-4846-b7bc-30f7f529e227",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creamos el modelo\n",
    "model = ConvNet3() #ConvNet1 #ConvNet2\n",
    "model.to(device)\n",
    "\n",
    "summary(model,\n",
    "        input_size=(BATCH_SIZE, 1, IMAGE_SIZE, IMAGE_SIZE),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff0c1ce-d7e4-4869-bfb4-77bd40a46a48",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ff0c1ce-d7e4-4869-bfb4-77bd40a46a48",
    "outputId": "1c4ebc48-6d53-4ce2-96b0-f7c220cc6b50",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)\n",
    "\n",
    "# bucle principal\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "for t in range(EPOCHS):\n",
    "    print(f\"Epoch {t+1}/{EPOCHS}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer, train_loss, train_acc)\n",
    "    test(test_dataloader, model, loss_fn, test_loss, test_acc)\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "torch.save(model.state_dict(), MODELS_DIR+\"model_cnn1.pth\")\n",
    "print(\"Model saved model_cnn1.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff0061c-a367-4758-beeb-04628e897833",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 933
    },
    "id": "eff0061c-a367-4758-beeb-04628e897833",
    "outputId": "453fde04-853f-4203-a67d-1e165b268aa4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(train_loss[-1])\n",
    "print(train_acc[-1])\n",
    "print(test_acc[-1])\n",
    "\n",
    "draw_graphics(train_loss, train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ac2bba-db9f-4961-b294-4b57715c4cad",
   "metadata": {
    "id": "48ac2bba-db9f-4961-b294-4b57715c4cad",
    "tags": []
   },
   "source": [
    "## Entrenamiento con ResNet18 desde Cero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad488cc-6808-446c-8450-4cf6a1ffbe32",
   "metadata": {
    "id": "9ad488cc-6808-446c-8450-4cf6a1ffbe32",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "from torchvision.transforms import Grayscale, ToTensor\n",
    "\n",
    "model = resnet18()\n",
    "\n",
    "transform = Compose([\n",
    "    Grayscale(num_output_channels=3),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "training = datasets.MNIST('data', train=True, download=True,\n",
    "                          transform=transform )\n",
    "testing = datasets.MNIST('data', train=False, download=True,\n",
    "                      transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(training, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(testing, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "# Cambiamos la capa superior con una capa Lineal con el número de clases\n",
    "num_features = model.fc.in_features\n",
    "model.fc = torch.nn.Sequential(\n",
    "    torch.nn.Dropout(p=0.5, inplace=True),\n",
    "    torch.nn.Linear(in_features=num_features,\n",
    "                    out_features=num_classes,\n",
    "                    bias=True)).to(device)\n",
    "\n",
    "summary(model,\n",
    "        input_size=(BATCH_SIZE, 3, IMAGE_SIZE, IMAGE_SIZE),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0148bf1-b282-4aab-a83f-df345ba02e2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "epochs = EPOCHS\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}/{epochs}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer, train_loss, train_acc)\n",
    "    test(test_dataloader, model, loss_fn, test_loss, test_acc)\n",
    "print(\"Done!\")\n",
    "\n",
    "torch.save(model.state_dict(), MODELS_DIR+\"model_resnet.pth\")\n",
    "print(\"Model saved to model_resnet.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b303291d-6294-4e5a-a31b-f39ae6431ce2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(train_loss[-1])\n",
    "print(train_acc[-1])\n",
    "print(test_acc[-1])\n",
    "\n",
    "draw_graphics(train_loss, train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5e83a4-d612-44f6-9fa0-27b960161494",
   "metadata": {
    "id": "0c5e83a4-d612-44f6-9fa0-27b960161494",
    "tags": []
   },
   "source": [
    "## Entrenamiento con ResNet18 y Transfer Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb6dc4e-9fca-4cf6-9219-c48e3ef24a29",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 859
    },
    "id": "4bb6dc4e-9fca-4cf6-9219-c48e3ef24a29",
    "outputId": "7f67f460-9140-46d6-c8df-e3bee2dab9f4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torchvision.transforms import Compose, Grayscale\n",
    "import numpy as np\n",
    "\n",
    "# Instanciamos el modelo ResNet18 con los pesos por defecto\n",
    "weights=ResNet18_Weights.DEFAULT\n",
    "auto_transforms = weights.transforms()\n",
    "\n",
    "# Cargamos el dataset con las transformaciones originales\n",
    "transform=Compose([\n",
    "    Grayscale(num_output_channels=3),\n",
    "    auto_transforms,\n",
    "])\n",
    "\n",
    "training = datasets.MNIST('data', train=True, download=True,\n",
    "                          transform=transform)\n",
    "testing = datasets.MNIST('data', train=False, download=True,\n",
    "                      transform=transform)\n",
    "\n",
    "# Seleccionamos un subconjunto de imágenes de entrenamiento \n",
    "reduced_training = Subset(training, range(2500))\n",
    "\n",
    "train_dataloader = DataLoader(reduced_training, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(testing, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Estructura de los batches\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "\n",
    "# Creamos el modelo y congelamos parámetros de la red\n",
    "model = resnet18(weights=weights).to(device)\n",
    "for name, para in model.named_parameters():\n",
    "    para.requires_grad = False\n",
    "\n",
    "# Cambiamos la capa superior con una capa Lineal con 10 clases de salida\n",
    "num_features = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(in_features=num_features,\n",
    "                    out_features=num_classes,\n",
    "                    bias=True).to(device)\n",
    "\n",
    "summary(model,\n",
    "        input_size=(BATCH_SIZE, 3, IMAGE_SIZE, IMAGE_SIZE),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60a6a67-de58-41c5-82d8-2c6494fddb50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "epochs = EPOCHS\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}/{epochs}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer, train_loss, train_acc)\n",
    "    test(test_dataloader, model, loss_fn, test_loss, test_acc)\n",
    "print(\"Done!\")\n",
    "\n",
    "torch.save(model.state_dict(), MODELS_DIR+\"model_resnet_tl1.pth\")\n",
    "print(\"Model saved to model_resnet_tl1.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8282ec01-81f0-4f16-bc9a-724867908b9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(train_loss[-1])\n",
    "print(train_acc[-1])\n",
    "print(test_acc[-1])\n",
    "\n",
    "draw_graphics(train_loss, train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6972227e-3e8e-4ee7-b20b-aaa7f1048bcc",
   "metadata": {
    "id": "6972227e-3e8e-4ee7-b20b-aaa7f1048bcc"
   },
   "source": [
    "### Mejoramos el resultado entrenando la red completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09765636-ce89-4414-90d9-c0e4c81862a3",
   "metadata": {
    "id": "09765636-ce89-4414-90d9-c0e4c81862a3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "for name, para in model.named_parameters():\n",
    "    para.requires_grad = True\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}/{epochs}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer, train_loss, train_acc)\n",
    "    test(test_dataloader, model, loss_fn, test_loss, test_acc)\n",
    "print(\"Done!\")\n",
    "\n",
    "torch.save(model.state_dict(), MODELS_DIR+\"model_resnet_tl2.pth\")\n",
    "print(\"Model saved to model_resnet_tl2.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1033a21a-ce33-4455-8ce7-d3f237c46e4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(train_loss[-1])\n",
    "print(train_acc[-1])\n",
    "print(test_acc[-1])\n",
    "\n",
    "draw_graphics(train_loss, train_acc, test_acc)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
